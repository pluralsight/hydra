application.name = hydra_test

akka {
  actor.provider = local
  loggers = ["akka.event.Logging$DefaultLogger"]
  loglevel = "ERROR"
  logger-startup-timeout = 30s
  persistence.journal.plugin = "akka.persistence.journal.inmem"
  persistence.snapshot-store.plugin = "akka.persistence.snapshot-store.local"

}

hydra_test {
  actors {
    kafka {
      consumer_proxy.path = "/user/kafka_consumer_proxy_test"
    }
  }
  schema.registry.url = "mock"
  transports.kafka.path = /user/kafka_producer
  transports.kafka.metrics.enabled = true
  transports.kafka.metrics.topic = "transport_test"
  kafka {
    supervisor.path = /system/kafka_producer_actor-2
    producer {
      type = "async"
      acks = 1
      retries = 0
      batch.size = 0 //disable
      metadata.fetch.timeout.ms = 10000
      max.block.ms = 10000
      message.send.max.retries = 0
      bootstrap.servers = "localhost:8092"
    }

    consumer {
      bootstrap.servers = "localhost:8092"
      zookeeper.connect = "localhost:3181"
      group.id = "hydra-group"
      metadata.fetch.timeout.ms = 100000
    }

    formats {
      string {
        key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
        key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
        value.serializer = "org.apache.kafka.common.serialization.StringSerializer"
        value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
        client.id = "hydra.string"
      }
      avro {
        key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
        key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
        value.serializer = "io.confluent.kafka.serializers.KafkaAvroSerializer"
        value.deserializer = "io.confluent.kafka.serializers.KafkaAvroDeserializer"
        client.id = "hydra.avro"
      }
    }
  }
}