akka {
  kafka.producer {
    # Tuning parameter of how many sends that can run in parallel.
    parallelism = 100

    # How long to wait for `KafkaProducer.close`
    close-timeout = 60s

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the producer stages. Some blocking may occur.
    # When this value is empty, the dispatcher configured for the stream
    # will be used.
    use-dispatcher = "akka.kafka.default-dispatcher"

    eos-commit-interval = 100ms

    # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
    # can be defined in this configuration section.
    kafka-clients {
    }
  }

  kafka.consumer {
    # Tuning property of scheduled polls.
    poll-interval = 500ms

    # Tuning property of the `KafkaConsumer.poll` parameter.
    # Note that non-zero value means that blocking of the thread that
    # is executing the stage will be blocked.
    poll-timeout = 50ms

    # The stage will be await outstanding offset commit requests before
    # shutting down, but if that takes longer than this timeout it will
    # stop forcefully.
    stop-timeout = 30s

    # How long to wait for `KafkaConsumer.close`
    close-timeout = 20s

    # If offset commit requests are not completed within this timeout
    # the returned Future is completed `TimeoutException`.
    commit-timeout = 15s

    # If the KafkaConsumer can't connect to the broker the poll will be
    # aborted after this timeout. The KafkaConsumerActor will throw
    # org.apache.kafka.common.errors.WakeupException which will be ignored
    # until max-wakeups limit gets exceeded.
    wakeup-timeout = 3s

    # After exceeding maxinum wakeups the consumer will stop and the stage will fail.
    max-wakeups = 10

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the KafkaConsumerActor. Some blocking may occur.
    use-dispatcher = "akka.kafka.default-dispatcher"

  }
}



hydra {
  kafka-ingestor-path = "/user/service/ingestor_registry/kafka_ingestor"
  kafka-ingestor-timeout = 1s

  bootstrap-config {
    partitions = 1
    partitions = ${?KAFKA_BROKER_NUM_PARTITIONS}
    replication-factor = 1
    replication-factor = ${?KAFKA_BROKER_REPLICATION_FACTOR}
    metadata-topic-name = "_hydra.metadata.topic"
    timeout = 3000
    failure-retry-millis = 5000
    failure-retry-millis = ${?BOOTSTRAP_FAILURE_RETRY_MILLIS}

    poll-interval = 50ms
    poll-timeout = 50ms
    stop-timeout = 30s
    close-timeout = 20s
    commit-timeout = 15s
    wakeup-timeout = 10s
    commit-time-warning = 20s
    wakeup-debug = true
    commit-refresh-interval = infinite
    max-wakeups = 2
    use-dispatcher = "akka.kafka.default-dispatcher"
    wait-close-partition = 500ms
    position-timeout = 5s
    offset-for-times-timeout = 5s
    metadata-request-timeout = 5s
    eos-draining-check-interval = 30ms
    partition-handler-warning = 5s
    connection-checker {
      enable = false
      max-retries = 3
      check-interval = 15s
      backoff-factor = 2.0
    }

    kafka-clients {
      enable.auto.commit = false
      key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      value.deserializer = io.confluent.kafka.serializers.KafkaAvroDeserializer
    }

  }

  kafka {
    health_check.interval = 30s
    heath_check.interval = ${?HYDRA_KAFKA_HEALTH_CHECK_INTERVAL}
    consumer {
      bootstrap.servers = "localhost:29092"
      bootstrap.servers = ${?HYDRA_KAFKA_PRODUCER_BOOTSTRAP_SERVERS}
      key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      group.id = "hydra"
      zookeeper.connect = "localhost:2181"
      zookeeper.connect = ${?HYDRA_ZOOKEEPER_QUORUM}
      auto.offset.reset = latest
    }

    admin {
      bootstrap.servers = "localhost:29092"
      bootstrap.servers = ${?HYDRA_KAFKA_PRODUCER_BOOTSTRAP_SERVERS}
    }

    producer {
      bootstrap.servers = "localhost:29092"
      bootstrap.servers = ${?HYDRA_KAFKA_PRODUCER_BOOTSTRAP_SERVERS}
      key.serializer = org.apache.kafka.common.serialization.StringSerializer
      max.in.flight.requests.per.connection = 1
    }

    clients {
      string.producer {
        value.serializer = org.apache.kafka.common.serialization.StringSerializer
      }
      string.consumer {
        value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      }
      avro.producer {
        value.serializer = "io.confluent.kafka.serializers.KafkaAvroSerializer"
        schema.registry.url = ${hydra.schema.registry.url}
        max.schemas.per.subject = ${hydra.max.schemas.per.subject}
      }
      avro.consumer {
        value.deserializer = "io.confluent.kafka.serializers.KafkaAvroDeserializer"
        schema.registry.url = ${hydra.schema.registry.url}
        max.schemas.per.subject = ${hydra.max.schemas.per.subject}
      }
      avrokey.producer {
        value.serializer = "io.confluent.kafka.serializers.KafkaAvroSerializer"
        key.serializer = "io.confluent.kafka.serializers.KafkaAvroSerializer"
        schema.registry.url = ${hydra.schema.registry.url}
        max.schemas.per.subject = ${hydra.max.schemas.per.subject}
      }
      json.producer {
        value.serializer = org.apache.kafka.common.serialization.StringSerializer
      }
      json.consumer {
        value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      }
    }
  }

}

