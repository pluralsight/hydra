akka {
  kafka.producer {
    # Tuning parameter of how many sends that can run in parallel.
    parallelism = 100

    # How long to wait for `KafkaProducer.close`
    close-timeout = 60s

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the producer stages. Some blocking may occur.
    # When this value is empty, the dispatcher configured for the stream
    # will be used.
    use-dispatcher = "akka.kafka.default-dispatcher"

    eos-commit-interval = 100ms

    # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
    # can be defined in this configuration section.
    kafka-clients {
        security.protocol=SASL_SSL
        sasl.jaas.config="org.apache.kafka.common.security.plain.PlainLoginModule   required username='I2TGTNT5E3SI6NX2'   password='5KI8zsJ47L0RZyE7fadOWTdwXS7s0VCHpAW2IJfd5cbJJxrLFAJz+SnJJhf4Vcjs';"
        sasl.mechanism=PLAIN
#         Required for correctness in Apache Kafka clients prior to 2.6
        client.dns.lookup=use_all_dns_ips

        schema.registry.url="https://psrc-vn38j.us-east-2.aws.confluent.cloud"
        basic.auth.credentials.source=USER_INFO
        basic.auth.user.info="PTGHGC2VS2HZEMJX:OLGqr67k2CY6/MO8lOnDdkqRkAJMFwpWu49t74f9yuJFHc/Ld+Kley6aWVhRm4Od"
    }

    security.protocol=SASL_SSL
    sasl.jaas.config="org.apache.kafka.common.security.plain.PlainLoginModule   required username='I2TGTNT5E3SI6NX2'   password='5KI8zsJ47L0RZyE7fadOWTdwXS7s0VCHpAW2IJfd5cbJJxrLFAJz+SnJJhf4Vcjs';"
    sasl.mechanism=PLAIN
#     Required for correctness in Apache Kafka clients prior to 2.6
    client.dns.lookup=use_all_dns_ips

    schema.registry.url="https://psrc-vn38j.us-east-2.aws.confluent.cloud"
    basic.auth.credentials.source=USER_INFO
    basic.auth.user.info="PTGHGC2VS2HZEMJX:OLGqr67k2CY6/MO8lOnDdkqRkAJMFwpWu49t74f9yuJFHc/Ld+Kley6aWVhRm4Od"
    # Best practice for higher availability in Apache Kafka clients prior to 3.0
    session.timeout.ms=45000
  }

  kafka.consumer {
    # Tuning property of scheduled polls.
    poll-interval = 500ms

    # Tuning property of the `KafkaConsumer.poll` parameter.
    # Note that non-zero value means that blocking of the thread that
    # is executing the stage will be blocked.
    poll-timeout = 50ms

    # The stage will be await outstanding offset commit requests before
    # shutting down, but if that takes longer than this timeout it will
    # stop forcefully.
    stop-timeout = 30s

    # How long to wait for `KafkaConsumer.close`
    close-timeout = 20s

    # If offset commit requests are not completed within this timeout
    # the returned Future is completed `TimeoutException`.
    commit-timeout = 15s

    # If the KafkaConsumer can't connect to the broker the poll will be
    # aborted after this timeout. The KafkaConsumerActor will throw
    # org.apache.kafka.common.errors.WakeupException which will be ignored
    # until max-wakeups limit gets exceeded.
    wakeup-timeout = 3s

    # After exceeding maxinum wakeups the consumer will stop and the stage will fail.
    max-wakeups = 10

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the KafkaConsumerActor. Some blocking may occur.
    use-dispatcher = "akka.kafka.default-dispatcher"

    kafka-clients {
        security.protocol=SASL_SSL
        sasl.jaas.config="org.apache.kafka.common.security.plain.PlainLoginModule   required username='I2TGTNT5E3SI6NX2'   password='5KI8zsJ47L0RZyE7fadOWTdwXS7s0VCHpAW2IJfd5cbJJxrLFAJz+SnJJhf4Vcjs';"
        sasl.mechanism=PLAIN
        # Required for correctness in Apache Kafka clients prior to 2.6
        client.dns.lookup=use_all_dns_ips

        schema.registry.url="https://psrc-vn38j.us-east-2.aws.confluent.cloud"
        basic.auth.credentials.source=USER_INFO
        basic.auth.user.info="PTGHGC2VS2HZEMJX:OLGqr67k2CY6/MO8lOnDdkqRkAJMFwpWu49t74f9yuJFHc/Ld+Kley6aWVhRm4Od"
        }

    security.protocol=SASL_SSL
    sasl.jaas.config="org.apache.kafka.common.security.plain.PlainLoginModule   required username='I2TGTNT5E3SI6NX2'   password='5KI8zsJ47L0RZyE7fadOWTdwXS7s0VCHpAW2IJfd5cbJJxrLFAJz+SnJJhf4Vcjs';"
    sasl.mechanism=PLAIN
#     Required for correctness in Apache Kafka clients prior to 2.6
    client.dns.lookup=use_all_dns_ips

    schema.registry.url="https://psrc-vn38j.us-east-2.aws.confluent.cloud"
    basic.auth.credentials.source=USER_INFO
    basic.auth.user.info="PTGHGC2VS2HZEMJX:OLGqr67k2CY6/MO8lOnDdkqRkAJMFwpWu49t74f9yuJFHc/Ld+Kley6aWVhRm4Od"
  }
}


hydra {
  kafka-ingestor-path = "/user/service/ingestor_registry/kafka_ingestor"
  kafka-ingestor-timeout = 1s

  sisurity {
    security.protocol=SASL_SSL
    sasl.jaas.config="org.apache.kafka.common.security.plain.PlainLoginModule   required username='I2TGTNT5E3SI6NX2'   password='5KI8zsJ47L0RZyE7fadOWTdwXS7s0VCHpAW2IJfd5cbJJxrLFAJz+SnJJhf4Vcjs';"
    sasl.mechanism=PLAIN
    basic.auth.credentials.source=USER_INFO
    basic.auth.user.info="PTGHGC2VS2HZEMJX:OLGqr67k2CY6/MO8lOnDdkqRkAJMFwpWu49t74f9yuJFHc/Ld+Kley6aWVhRm4Od"
    schema.registry.basic.auth.user.info="PTGHGC2VS2HZEMJX:OLGqr67k2CY6/MO8lOnDdkqRkAJMFwpWu49t74f9yuJFHc/Ld+Kley6aWVhRm4Od"
    # Required for correctness in Apache Kafka clients prior to 2.6
    client.dns.lookup=use_all_dns_ips

    # Best practice for higher availability in Apache Kafka clients prior to 3.0
    session.timeout.ms=45000
  }

  bootstrap-config = ${hydra.sisurity}

  bootstrap-config {
    partitions = 1
    partitions = ${?KAFKA_BROKER_NUM_PARTITIONS}
    replication-factor = 1
    replication-factor = ${?KAFKA_BROKER_REPLICATION_FACTOR}
    min-insync-replicas = 1
    min-insync-replicas = ${?KAFKA_BROKER_MIN_INSYNC_REPLICAS}
    metadata-topic-name = "_hydra.metadata.topic"
    timeout = 3000
    failure-retry-millis = 5000
    failure-retry-millis = ${?BOOTSTRAP_FAILURE_RETRY_MILLIS}

    poll-interval = 50ms
    poll-timeout = 50ms
    stop-timeout = 30s
    close-timeout = 20s
    commit-timeout = 15s
    wakeup-timeout = 10s
    commit-time-warning = 20s
    wakeup-debug = true
    commit-refresh-interval = infinite
    max-wakeups = 2
    use-dispatcher = "akka.kafka.default-dispatcher"
    wait-close-partition = 500ms
    position-timeout = 5s
    offset-for-times-timeout = 5s
    metadata-request-timeout = 5s
    eos-draining-check-interval = 30ms
    partition-handler-warning = 5s
    connection-checker {
      enable = false
      max-retries = 3
      check-interval = 15s
      backoff-factor = 2.0
    }

    kafka-clients {
      enable.auto.commit = false
      key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      value.deserializer = io.confluent.kafka.serializers.KafkaAvroDeserializer
    }

  }

  kafka.consumer = ${hydra.sisurity}
  kafka.admin = ${hydra.sisurity}
  kafka.producer = ${hydra.sisurity}

  kafka {
    health_check.interval = 30s
    heath_check.interval = ${?HYDRA_KAFKA_HEALTH_CHECK_INTERVAL}
    consumer {
      bootstrap.servers = "localhost:29092"
      bootstrap.servers = ${?HYDRA_KAFKA_PRODUCER_BOOTSTRAP_SERVERS}
      key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      group.id = "hydra"
      auto.offset.reset = latest
    }

    admin {
      bootstrap.servers = "localhost:29092"
      bootstrap.servers = ${?HYDRA_KAFKA_PRODUCER_BOOTSTRAP_SERVERS}
    }

    producer {
      bootstrap.servers = "localhost:29092"
      bootstrap.servers = ${?HYDRA_KAFKA_PRODUCER_BOOTSTRAP_SERVERS}

      # Best practice for higher availability in Apache Kafka clients prior to 3.0
      session.timeout.ms=45000
      key.serializer = org.apache.kafka.common.serialization.StringSerializer
      max.in.flight.requests.per.connection = 1
    }

    clients {
      string.producer = ${hydra.sisurity}
      avro.producer = ${hydra.sisurity}
      avrokey.producer = ${hydra.sisurity}
      json.producer = ${hydra.sisurity}

      json.consumer = ${hydra.sisurity}
      string.consumer = ${hydra.sisurity}


      string.producer {
        value.serializer = org.apache.kafka.common.serialization.StringSerializer
      }
      string.consumer {
        value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      }
      avro.producer {
        value.serializer = "io.confluent.kafka.serializers.KafkaAvroSerializer"
        schema.registry.url = ${hydra.schema.registry.url}
        max.schemas.per.subject = ${hydra.max.schemas.per.subject}
      }
      avro.consumer {
        value.deserializer = "io.confluent.kafka.serializers.KafkaAvroDeserializer"
        schema.registry.url = ${hydra.schema.registry.url}
        max.schemas.per.subject = ${hydra.max.schemas.per.subject}
      }
      avrokey.producer {
        value.serializer = "io.confluent.kafka.serializers.KafkaAvroSerializer"
        key.serializer = "io.confluent.kafka.serializers.KafkaAvroSerializer"
        schema.registry.url = ${hydra.schema.registry.url}
        max.schemas.per.subject = ${hydra.max.schemas.per.subject}
      }
      json.producer {
        value.serializer = org.apache.kafka.common.serialization.StringSerializer
      }
      json.consumer {
        value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      }
    }
  }

}
