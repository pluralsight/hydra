application.name = hydra_kafka


akka {
  remote {
    artery {
      canonical.port = 0
    }
    netty.tcp.port = 0
  }
  //actor.provider = local
  loggers = ["akka.event.Logging$DefaultLogger"]
  loglevel = "ERROR"
  logger-startup-timeout = 30s
  persistence.journal.plugin = "akka.persistence.journal.inmem"
}


hydra_kafka {
  http.port = 8080
  http.interface = 0.0.0.0
  kafka-ingestor-path = "/user/ingestor_registry/kafka_ingestor"

  bootstrap-config {
    eos-draining-check-interval = 30ms
    connection-checker {

      #Flag to turn on connection checker
      enable = false

      # Amount of attempts to be performed after a first connection failure occurs
      # Required, non-negative integer
      max-retries = 3

      # Interval for the connection check. Used as the base for exponential retry.
      check-interval = 15s

      # Check interval multiplier for backoff interval
      # Required, positive number
      backoff-factor = 2.0
    }
    partition-handler-warning = 5s
    partitions = 1
    replication-factor = 1
    metadata-topic-name = "_hydra.metadata.topic"
    timeout = 3000
    failure-retry-millis = 3000

    poll-interval = 50ms
    poll-timeout = 50ms
    stop-timeout = 30s
    close-timeout = 20s
    commit-timeout = 15s
    wakeup-timeout = 10s
    commit-time-warning = 20s
    wakeup-debug = true
    commit-refresh-interval = infinite
    max-wakeups = 2
    use-dispatcher = "akka.kafka.default-dispatcher"
    wait-close-partition = 500ms
    position-timeout = 5s
    offset-for-times-timeout = 5s
    metadata-request-timeout = 5s

    kafka-clients {
      enable.auto.commit = false
      key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      value.deserializer = io.confluent.kafka.serializers.KafkaAvroDeserializer
    }
  }

  actors {
    kafka {
      consumer_proxy.path = "/user/kafka_consumer_proxy_test"
    }
  }

  schema.registry.url = "mock"
  transports.kafka.path = /user/kafka_producer
  transports.kafka.metrics.enabled = true
  transports.kafka.metrics.topic = "transport_test"

  kafka {
    supervisor.path = /system/kafka_producer_actor-2
    producer {
      type = "async"
      acks = 1
      retries = 0
      batch.size = 0 //disable
      metadata.fetch.timeout.ms = 10000
      max.block.ms = 10000
      message.send.max.retries = 0
      bootstrap.servers = "localhost:8012"
      key.serializer = org.apache.kafka.common.serialization.StringSerializer
    }

    admin {
      bootstrap.servers = "localhost:8012"
    }

    consumer {
      bootstrap.servers = "localhost:8012"
      zookeeper.connect = "localhost:3111"
      group.id = "hydra"
      metadata.fetch.timeout.ms = 100000
      key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      auto.offset.reset = latest
    }

    clients {
      string.producer {
        value.serializer = org.apache.kafka.common.serialization.StringSerializer
      }
      string.consumer {
        value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      }
      avro.producer {
        value.serializer = "io.confluent.kafka.serializers.KafkaAvroSerializer"
        schema.registry.url = ${hydra_kafka.schema.registry.url}
      }
      avro.consumer {
        value.deserializer = "io.confluent.kafka.serializers.KafkaAvroDeserializer"
        schema.registry.url = ${hydra_kafka.schema.registry.url}
      }
      json.producer {
        value.serializer = org.apache.kafka.common.serialization.StringSerializer
      }
      json.consumer {
        value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      }
      tester.producer {
        key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
        value.serializer = "org.apache.kafka.common.serialization.StringSerializer"
        metadata.fetch.timeout.ms = 1000
      }
    }
  }
}